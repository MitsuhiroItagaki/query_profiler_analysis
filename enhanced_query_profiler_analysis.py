#!/usr/bin/env python3
"""
Enhanced Databricks SQL Profiler Analysis Tool with Shuffle Optimization

This tool analyzes Databricks SQL profiler JSON files and provides:
1. Standard performance analysis
2. Enhanced Shuffle operation validation
3. Memory efficiency per partition analysis  
4. Optimization recommendations based on memory/partition ratio

Key Enhancement: Validates that peak memory per partition is â‰¤ 512MB
"""

import json
import re
import os
import sys
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple, Union
import traceback

# Shuffle Analysis Configuration
SHUFFLE_ANALYSIS_CONFIG = {
    # Memory per partition threshold (512MB in bytes)
    "memory_per_partition_threshold_mb": 512,
    "memory_per_partition_threshold_bytes": 512 * 1024 * 1024,
    
    # Minimum partition count recommendations  
    "min_partition_count": 1,
    "max_partition_count": 2000,
    
    # Performance thresholds
    "high_memory_threshold_gb": 100,  # GB
    "long_execution_threshold_sec": 300,  # 5 minutes
    
    # Optimization recommendations
    "enable_liquid_clustering_advice": True,
    "enable_partition_tuning_advice": True,
    "enable_cluster_sizing_advice": True
}

def analyze_shuffle_operations(node_metrics: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Shuffleæ“ä½œã®å¦¥å½“æ€§ã‚’åˆ†æã—ã€æœ€é©åŒ–æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆã™ã‚‹
    
    Key validation: ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª/ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•° â‰¤ 512MB
    
    Args:
        node_metrics: ãƒãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆ
        
    Returns:
        dict: Shuffleåˆ†æçµæœ
            {
                "shuffle_nodes": [
                    {
                        "node_id": str,
                        "node_name": str, 
                        "partition_count": int,
                        "peak_memory_bytes": int,
                        "peak_memory_gb": float,
                        "memory_per_partition_mb": float,
                        "is_memory_efficient": bool,  # True if â‰¤ 512MB per partition
                        "duration_sec": float,
                        "rows_processed": int,
                        "optimization_priority": str,  # "HIGH", "MEDIUM", "LOW"
                        "recommendations": [str]
                    }
                ],
                "overall_assessment": {
                    "total_shuffle_nodes": int,
                    "inefficient_nodes": int,
                    "total_memory_gb": float,
                    "avg_memory_per_partition_mb": float,
                    "needs_optimization": bool,
                    "optimization_summary": [str]
                }
            }
    """
    
    shuffle_analysis = {
        "shuffle_nodes": [],
        "overall_assessment": {
            "total_shuffle_nodes": 0,
            "inefficient_nodes": 0,
            "total_memory_gb": 0.0,
            "avg_memory_per_partition_mb": 0.0,
            "needs_optimization": False,
            "optimization_summary": []
        }
    }
    
    threshold_mb = SHUFFLE_ANALYSIS_CONFIG["memory_per_partition_threshold_mb"]
    threshold_bytes = SHUFFLE_ANALYSIS_CONFIG["memory_per_partition_threshold_bytes"]
    
    total_memory_bytes = 0
    total_partitions = 0
    inefficient_count = 0
    
    # Shuffleæ“ä½œã‚’æŒã¤ãƒãƒ¼ãƒ‰ã‚’ç‰¹å®š
    for node in node_metrics:
        node_name = node.get('name', '')
        node_tag = node.get('tag', '')
        node_id = node.get('node_id', node.get('id', ''))
        
        # Shuffleæ“ä½œãƒãƒ¼ãƒ‰ã‹ã©ã†ã‹ã‚’åˆ¤å®š
        if 'SHUFFLE' not in node_tag:
            continue
            
        # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã‚’å–å¾—
        partition_count = 0
        peak_memory_bytes = 0
        duration_ms = 0
        rows_processed = 0
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã¨ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒªã‚’æŠ½å‡º
        detailed_metrics = node.get('detailed_metrics', {})
        key_metrics = node.get('key_metrics', {})
        raw_metrics = node.get('metrics', [])
        
        # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã®å–å¾— - è¤‡æ•°ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰æ¤œç´¢
        partition_metrics = [
            "Sink - Number of partitions",
            "Number of partitions", 
            "AQEShuffleRead - Number of partitions"
        ]
        
        # detailed_metricsã‹ã‚‰æ¤œç´¢
        for metric_name in partition_metrics:
            if metric_name in detailed_metrics:
                partition_count = detailed_metrics[metric_name].get('value', 0)
                break
        
        # raw_metricsã‹ã‚‰ã‚‚æ¤œç´¢
        if partition_count == 0 and isinstance(raw_metrics, list):
            for metric in raw_metrics:
                metric_label = metric.get('label', '')
                if metric_label in partition_metrics:
                    partition_count = metric.get('value', 0)
                    break
        
        # ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒªã®å–å¾—
        peak_memory_bytes = key_metrics.get('peakMemoryBytes', 0)
        if peak_memory_bytes == 0:
            peak_memory_bytes = key_metrics.get('peak_memory_bytes', 0)
        
        duration_ms = key_metrics.get('durationMs', 0)
        rows_processed = key_metrics.get('rowsNum', 0)
        if rows_processed == 0:
            rows_processed = key_metrics.get('rows_num', 0)
        
        # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ãŒ0ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if partition_count <= 0:
            continue
            
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§ã®è¨ˆç®—
        memory_per_partition_bytes = peak_memory_bytes / partition_count
        memory_per_partition_mb = memory_per_partition_bytes / (1024 * 1024)
        peak_memory_gb = peak_memory_bytes / (1024 * 1024 * 1024)
        duration_sec = duration_ms / 1000.0
        
        # 512MBåŸºæº–ã§ã®åŠ¹ç‡æ€§åˆ¤å®š
        is_memory_efficient = memory_per_partition_mb <= threshold_mb
        
        # æœ€é©åŒ–å„ªå…ˆåº¦ã®åˆ¤å®š
        optimization_priority = "LOW"
        if memory_per_partition_mb > threshold_mb * 4:  # 2GBä»¥ä¸Š
            optimization_priority = "HIGH"
        elif memory_per_partition_mb > threshold_mb * 2:  # 1GBä»¥ä¸Š
            optimization_priority = "HIGH"
        elif memory_per_partition_mb > threshold_mb:  # 512MB-1GB
            optimization_priority = "MEDIUM"
        
        # æ¨å¥¨äº‹é …ã®ç”Ÿæˆ
        recommendations = []
        
        if not is_memory_efficient:
            inefficient_count += 1
            
            # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã®æ¨å¥¨è¨ˆç®—
            optimal_partition_count = int((memory_per_partition_mb / threshold_mb) * partition_count)
            
            if memory_per_partition_mb > threshold_mb * 4:  # 2GBä»¥ä¸Š
                recommendations.append(
                    f"ğŸš¨ éå¸¸ã«é«˜ã„ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ ({memory_per_partition_mb:.0f}MB/ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³): "
                    f"ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã‚’{optimal_partition_count}ä»¥ä¸Šã«å¢—åŠ ã™ã‚‹ã‹ã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚µã‚¤ã‚ºã‚’æ‹¡å¼µã—ã¦ãã ã•ã„"
                )
                recommendations.append("ğŸ–¥ï¸ ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ‹¡å¼µ: ã‚ˆã‚Šå¤šãã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ã¾ãŸã¯é«˜ãƒ¡ãƒ¢ãƒªã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ä½¿ç”¨ã‚’æ¤œè¨")
            elif memory_per_partition_mb > threshold_mb * 2:  # 1GBä»¥ä¸Š  
                recommendations.append(
                    f"âš ï¸ é«˜ã„ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ ({memory_per_partition_mb:.0f}MB/ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³): "
                    f"ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã‚’{optimal_partition_count}ä»¥ä¸Šã«å¢—åŠ ã™ã‚‹ã“ã¨ã‚’å¼·ãæ¨å¥¨"
                )
                recommendations.append("âš™ï¸ AQEè¨­å®š: spark.sql.adaptive.advisoryPartitionSizeInBytes ã®èª¿æ•´ã‚’æ¤œè¨")
            else:
                recommendations.append(
                    f"ğŸ’¡ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ”¹å–„ ({memory_per_partition_mb:.0f}MB/ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³): "
                    f"ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã‚’{optimal_partition_count}ã«èª¿æ•´ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨"
                )
        
        if peak_memory_gb > SHUFFLE_ANALYSIS_CONFIG["high_memory_threshold_gb"]:
            recommendations.append(
                f"ğŸ”§ Liquid Clusteringã®å®Ÿè£…ã«ã‚ˆã‚Šã€Shuffleæ“ä½œã®å‰Šæ¸›ã‚’æ¤œè¨ "
                f"(ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {peak_memory_gb:.1f}GB)"
            )
            
        if duration_sec > SHUFFLE_ANALYSIS_CONFIG["long_execution_threshold_sec"]:
            recommendations.append(
                f"â±ï¸ å®Ÿè¡Œæ™‚é–“ãŒé•·ã„ ({duration_sec:.1f}ç§’): ãƒ‡ãƒ¼ã‚¿åˆ†æ•£æˆ¦ç•¥ã®è¦‹ç›´ã—ã‚’æ¨å¥¨"
            )
            
        if rows_processed > 1000000000:  # 10å„„è¡Œä»¥ä¸Š
            recommendations.append(
                f"ğŸ“Š å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç† ({rows_processed:,}è¡Œ): "
                "ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆJOINã‚„äº‹å‰é›†ç´„ã®æ´»ç”¨ã‚’æ¤œè¨"
            )
        
        # Shuffleåˆ†æçµæœã«è¿½åŠ 
        shuffle_node_analysis = {
            "node_id": node_id,
            "node_name": node_name,
            "partition_count": partition_count,
            "peak_memory_bytes": peak_memory_bytes,
            "peak_memory_gb": round(peak_memory_gb, 2),
            "memory_per_partition_mb": round(memory_per_partition_mb, 2),
            "is_memory_efficient": is_memory_efficient,
            "duration_sec": round(duration_sec, 2),
            "rows_processed": rows_processed,
            "optimization_priority": optimization_priority,
            "recommendations": recommendations
        }
        
        shuffle_analysis["shuffle_nodes"].append(shuffle_node_analysis)
        
        # å…¨ä½“çµ±è¨ˆã«è¿½åŠ 
        total_memory_bytes += peak_memory_bytes
        total_partitions += partition_count
    
    # å…¨ä½“è©•ä¾¡ã®è¨ˆç®—
    total_shuffle_nodes = len(shuffle_analysis["shuffle_nodes"])
    shuffle_analysis["overall_assessment"]["total_shuffle_nodes"] = total_shuffle_nodes
    shuffle_analysis["overall_assessment"]["inefficient_nodes"] = inefficient_count
    shuffle_analysis["overall_assessment"]["total_memory_gb"] = round(total_memory_bytes / (1024**3), 2)
    
    if total_partitions > 0:
        avg_memory_per_partition_mb = (total_memory_bytes / total_partitions) / (1024**2)
        shuffle_analysis["overall_assessment"]["avg_memory_per_partition_mb"] = round(avg_memory_per_partition_mb, 2)
    
    # æœ€é©åŒ–ã®å¿…è¦æ€§åˆ¤å®š
    needs_optimization = inefficient_count > 0
    shuffle_analysis["overall_assessment"]["needs_optimization"] = needs_optimization
    
    # å…¨ä½“çš„ãªæœ€é©åŒ–æ¨å¥¨äº‹é …
    optimization_summary = []
    
    if needs_optimization:
        efficiency_rate = ((total_shuffle_nodes - inefficient_count) / total_shuffle_nodes) * 100
        optimization_summary.append(
            f"ğŸ”§ {inefficient_count}/{total_shuffle_nodes} ã®Shuffleæ“ä½œã§æœ€é©åŒ–ãŒå¿…è¦ "
            f"(åŠ¹ç‡æ€§: {efficiency_rate:.1f}%)"
        )
        
        if SHUFFLE_ANALYSIS_CONFIG["enable_liquid_clustering_advice"]:
            optimization_summary.append(
                "ğŸ’ Liquid Clusteringã®å®Ÿè£…ã«ã‚ˆã‚Šæ ¹æœ¬çš„ãªShuffleå‰Šæ¸›ã‚’æ¨å¥¨ "
                "(æœ€ã‚‚åŠ¹æœçš„ãªé•·æœŸè§£æ±ºç­–)"
            )
            
        if SHUFFLE_ANALYSIS_CONFIG["enable_partition_tuning_advice"]:
            optimization_summary.append(
                "âš™ï¸ é©åˆ‡ãªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã¸ã®èª¿æ•´ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’æ”¹å–„ "
                f"(ç›®æ¨™: â‰¤{threshold_mb}MB/ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³)"
            )
            
        if SHUFFLE_ANALYSIS_CONFIG["enable_cluster_sizing_advice"]:
            optimization_summary.append(
                "ğŸ–¥ï¸ ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚µã‚¤ã‚ºã®æ‹¡å¼µã§ãƒ¡ãƒ¢ãƒªåœ§è¿«ã‚’è»½æ¸› "
                "(é«˜å„ªå…ˆåº¦ã‚±ãƒ¼ã‚¹ã§æ¨å¥¨)"
            )
    else:
        optimization_summary.append(
            f"âœ… ã™ã¹ã¦ã®Shuffleæ“ä½œã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒé©åˆ‡ã§ã™ "
            f"(â‰¤{threshold_mb}MB/ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³)"
        )
    
    shuffle_analysis["overall_assessment"]["optimization_summary"] = optimization_summary
    
    return shuffle_analysis

def generate_shuffle_optimization_report(shuffle_analysis: Dict[str, Any], output_language: str = 'ja') -> str:
    """
    Shuffleæœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    
    Args:
        shuffle_analysis: analyze_shuffle_operations()ã®çµæœ
        output_language: å‡ºåŠ›è¨€èª ('ja' or 'en')
        
    Returns:
        str: ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆ
    """
    
    if output_language == 'ja':
        report_lines = [
            "",
            "=" * 80,
            "ğŸ”§ SHUFFLEæ“ä½œæœ€é©åŒ–åˆ†æãƒ¬ãƒãƒ¼ãƒˆ",
            "=" * 80,
            f"ğŸ“Š åŸºæº–: ãƒ¡ãƒ¢ãƒª/ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ â‰¤ {SHUFFLE_ANALYSIS_CONFIG['memory_per_partition_threshold_mb']}MB",
            "=" * 80,
            ""
        ]
        
        overall = shuffle_analysis["overall_assessment"]
        
        # å…¨ä½“ã‚µãƒãƒªãƒ¼
        report_lines.extend([
            "ğŸ“Š å…¨ä½“ã‚µãƒãƒªãƒ¼:",
            f"  ãƒ»Shuffleæ“ä½œæ•°: {overall['total_shuffle_nodes']}",
            f"  ãƒ»æœ€é©åŒ–ãŒå¿…è¦ãªæ“ä½œ: {overall['inefficient_nodes']}",
            f"  ãƒ»ç·ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {overall['total_memory_gb']} GB",
            f"  ãƒ»å¹³å‡ãƒ¡ãƒ¢ãƒª/ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³: {overall['avg_memory_per_partition_mb']:.1f} MB",
            f"  ãƒ»æœ€é©åŒ–å¿…è¦æ€§: {'ã¯ã„' if overall['needs_optimization'] else 'ã„ã„ãˆ'}",
            ""
        ])
        
        # åŠ¹ç‡æ€§ã‚¹ã‚³ã‚¢
        if overall['total_shuffle_nodes'] > 0:
            efficiency_score = ((overall['total_shuffle_nodes'] - overall['inefficient_nodes']) / overall['total_shuffle_nodes']) * 100
            efficiency_icon = "ğŸŸ¢" if efficiency_score >= 80 else "ğŸŸ¡" if efficiency_score >= 60 else "ğŸ”´"
            report_lines.extend([
                f"ğŸ¯ ShuffleåŠ¹ç‡æ€§ã‚¹ã‚³ã‚¢: {efficiency_icon} {efficiency_score:.1f}%",
                ""
            ])
        
        # å€‹åˆ¥Shuffleåˆ†æ
        if shuffle_analysis["shuffle_nodes"]:
            report_lines.extend([
                "ğŸ” å€‹åˆ¥Shuffleæ“ä½œåˆ†æ:",
                ""
            ])
            
            # å„ªå…ˆåº¦é †ã«ã‚½ãƒ¼ãƒˆ
            sorted_nodes = sorted(
                shuffle_analysis["shuffle_nodes"], 
                key=lambda x: (
                    {"HIGH": 0, "MEDIUM": 1, "LOW": 2}[x["optimization_priority"]],
                    -x["memory_per_partition_mb"]
                )
            )
            
            for i, node in enumerate(sorted_nodes, 1):
                priority_icon = {"HIGH": "ğŸš¨", "MEDIUM": "âš ï¸", "LOW": "ğŸ’¡"}.get(node["optimization_priority"], "ğŸ“Š")
                efficiency_status = "âœ… åŠ¹ç‡çš„" if node["is_memory_efficient"] else "âŒ éåŠ¹ç‡"
                
                # ãƒ¡ãƒ¢ãƒª/ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®è­¦å‘Šãƒ¬ãƒ™ãƒ«
                memory_status = ""
                if node["memory_per_partition_mb"] > SHUFFLE_ANALYSIS_CONFIG["memory_per_partition_threshold_mb"] * 4:
                    memory_status = " ğŸ”¥ å±é™ºãƒ¬ãƒ™ãƒ«"
                elif node["memory_per_partition_mb"] > SHUFFLE_ANALYSIS_CONFIG["memory_per_partition_threshold_mb"] * 2:
                    memory_status = " âš ï¸ é«˜ãƒ¬ãƒ™ãƒ«"
                elif node["memory_per_partition_mb"] > SHUFFLE_ANALYSIS_CONFIG["memory_per_partition_threshold_mb"]:
                    memory_status = " ğŸ“ˆ è¦æ³¨æ„"
                
                report_lines.extend([
                    f"{i}. {node['node_name']} (Node ID: {node['node_id']})",
                    f"   {priority_icon} å„ªå…ˆåº¦: {node['optimization_priority']}",
                    f"   ğŸ“Š ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {node['partition_count']:,}",
                    f"   ğŸ§  ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª: {node['peak_memory_gb']} GB",
                    f"   âš¡ ãƒ¡ãƒ¢ãƒª/ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³: {node['memory_per_partition_mb']:.1f} MB{memory_status}",
                    f"   â±ï¸ å®Ÿè¡Œæ™‚é–“: {node['duration_sec']:.1f} ç§’",
                    f"   ğŸ“ˆ å‡¦ç†è¡Œæ•°: {node['rows_processed']:,}",
                    f"   ğŸ¯ åŠ¹ç‡æ€§: {efficiency_status}",
                    ""
                ])
                
                if node["recommendations"]:
                    report_lines.append("   ğŸ’¡ æ¨å¥¨äº‹é …:")
                    for rec in node["recommendations"]:
                        report_lines.append(f"     - {rec}")
                    report_lines.append("")
        
        # å…¨ä½“æœ€é©åŒ–æ¨å¥¨äº‹é …
        if overall["optimization_summary"]:
            report_lines.extend([
                "ğŸ¯ å…¨ä½“æœ€é©åŒ–æ¨å¥¨äº‹é …:",
                ""
            ])
            for summary in overall["optimization_summary"]:
                report_lines.append(f"  {summary}")
            report_lines.append("")
        
        # å…·ä½“çš„ãªå®Ÿè£…æ‰‹é †
        if overall["needs_optimization"]:
            report_lines.extend([
                "ğŸ“‹ å®Ÿè£…æ‰‹é † (å„ªå…ˆåº¦é †):",
                "",
                "1ï¸âƒ£ ç·Šæ€¥å¯¾ç­– (é«˜å„ªå…ˆåº¦ãƒãƒ¼ãƒ‰å‘ã‘):",
                "   - ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚µã‚¤ã‚ºã®æ‹¡å¼µ (ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰æ•°å¢—åŠ )",
                "   - é«˜ãƒ¡ãƒ¢ãƒªã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã¸ã®å¤‰æ›´",
                "   - spark.sql.adaptive.coalescePartitions.maxBatchSize ã®èª¿æ•´",
                "",
                "2ï¸âƒ£ çŸ­æœŸå¯¾ç­– (å³åº§ã«å®Ÿè¡Œå¯èƒ½):",
                "   - spark.sql.adaptive.coalescePartitions.enabled = true",
                "   - spark.sql.adaptive.skewJoin.enabled = true", 
                "   - spark.sql.adaptive.advisoryPartitionSizeInBytes ã®èª¿æ•´",
                f"   - ç›®æ¨™: {SHUFFLE_ANALYSIS_CONFIG['memory_per_partition_threshold_mb']}MB/ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ä»¥ä¸‹",
                "",
                "3ï¸âƒ£ ä¸­æœŸå¯¾ç­– (è¨ˆç”»çš„å®Ÿè£…):",
                "   - ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã®æ˜ç¤ºçš„æŒ‡å®š (.repartition())",
                "   - JOINæˆ¦ç•¥ã®æœ€é©åŒ– (ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆJOINã®æ´»ç”¨)",
                "   - ãƒ‡ãƒ¼ã‚¿åˆ†æ•£æˆ¦ç•¥ã®è¦‹ç›´ã—",
                "",
                "4ï¸âƒ£ é•·æœŸå¯¾ç­– (æ ¹æœ¬çš„è§£æ±º):",
                "   - Liquid Clusteringã®å®Ÿè£…",
                "   - ãƒ†ãƒ¼ãƒ–ãƒ«è¨­è¨ˆã®æœ€é©åŒ–",
                "   - ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰åˆ†é›¢ã®æ¤œè¨",
                ""
            ])
            
            # Sparkãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å¥¨å€¤
            if overall["avg_memory_per_partition_mb"] > SHUFFLE_ANALYSIS_CONFIG["memory_per_partition_threshold_mb"]:
                target_partition_size_mb = min(SHUFFLE_ANALYSIS_CONFIG["memory_per_partition_threshold_mb"], 256)
                target_partition_size_bytes = target_partition_size_mb * 1024 * 1024
                
                report_lines.extend([
                    "âš™ï¸ æ¨å¥¨Sparkãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:",
                    "",
                    f"spark.sql.adaptive.advisoryPartitionSizeInBytes = {target_partition_size_bytes}",
                    "spark.sql.adaptive.coalescePartitions.minPartitionNum = 1",
                    "spark.sql.adaptive.coalescePartitions.maxBatchSize = 100",
                    "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes = 268435456",
                    ""
                ])
        
    else:  # English
        report_lines = [
            "",
            "=" * 80,
            "ğŸ”§ SHUFFLE OPERATION OPTIMIZATION ANALYSIS REPORT",
            "=" * 80,
            f"ğŸ“Š Threshold: Memory per Partition â‰¤ {SHUFFLE_ANALYSIS_CONFIG['memory_per_partition_threshold_mb']}MB",
            "=" * 80,
            ""
        ]
        
        overall = shuffle_analysis["overall_assessment"]
        
        # Overall Summary
        report_lines.extend([
            "ğŸ“Š Overall Summary:",
            f"  â€¢ Number of Shuffle Operations: {overall['total_shuffle_nodes']}",
            f"  â€¢ Operations Requiring Optimization: {overall['inefficient_nodes']}",
            f"  â€¢ Total Memory Usage: {overall['total_memory_gb']} GB",
            f"  â€¢ Average Memory per Partition: {overall['avg_memory_per_partition_mb']:.1f} MB",
            f"  â€¢ Optimization Required: {'Yes' if overall['needs_optimization'] else 'No'}",
            ""
        ])
        
        # Efficiency Score
        if overall['total_shuffle_nodes'] > 0:
            efficiency_score = ((overall['total_shuffle_nodes'] - overall['inefficient_nodes']) / overall['total_shuffle_nodes']) * 100
            efficiency_icon = "ğŸŸ¢" if efficiency_score >= 80 else "ğŸŸ¡" if efficiency_score >= 60 else "ğŸ”´"
            report_lines.extend([
                f"ğŸ¯ Shuffle Efficiency Score: {efficiency_icon} {efficiency_score:.1f}%",
                ""
            ])
        
        # Individual Shuffle Analysis
        if shuffle_analysis["shuffle_nodes"]:
            report_lines.extend([
                "ğŸ” Individual Shuffle Operation Analysis:",
                ""
            ])
            
            # Sort by priority
            sorted_nodes = sorted(
                shuffle_analysis["shuffle_nodes"], 
                key=lambda x: (
                    {"HIGH": 0, "MEDIUM": 1, "LOW": 2}[x["optimization_priority"]],
                    -x["memory_per_partition_mb"]
                )
            )
            
            for i, node in enumerate(sorted_nodes, 1):
                priority_icon = {"HIGH": "ğŸš¨", "MEDIUM": "âš ï¸", "LOW": "ğŸ’¡"}.get(node["optimization_priority"], "ğŸ“Š")
                efficiency_status = "âœ… Efficient" if node["is_memory_efficient"] else "âŒ Inefficient"
                
                # Memory/partition warning level
                memory_status = ""
                if node["memory_per_partition_mb"] > SHUFFLE_ANALYSIS_CONFIG["memory_per_partition_threshold_mb"] * 4:
                    memory_status = " ğŸ”¥ Critical"
                elif node["memory_per_partition_mb"] > SHUFFLE_ANALYSIS_CONFIG["memory_per_partition_threshold_mb"] * 2:
                    memory_status = " âš ï¸ High"
                elif node["memory_per_partition_mb"] > SHUFFLE_ANALYSIS_CONFIG["memory_per_partition_threshold_mb"]:
                    memory_status = " ğŸ“ˆ Warning"
                
                report_lines.extend([
                    f"{i}. {node['node_name']} (Node ID: {node['node_id']})",
                    f"   {priority_icon} Priority: {node['optimization_priority']}",
                    f"   ğŸ“Š Partition Count: {node['partition_count']:,}",
                    f"   ğŸ§  Peak Memory: {node['peak_memory_gb']} GB",
                    f"   âš¡ Memory per Partition: {node['memory_per_partition_mb']:.1f} MB{memory_status}",
                    f"   â±ï¸ Execution Time: {node['duration_sec']:.1f} seconds",
                    f"   ğŸ“ˆ Rows Processed: {node['rows_processed']:,}",
                    f"   ğŸ¯ Efficiency: {efficiency_status}",
                    ""
                ])
                
                if node["recommendations"]:
                    report_lines.append("   ğŸ’¡ Recommendations:")
                    for rec in node["recommendations"]:
                        report_lines.append(f"     - {rec}")
                    report_lines.append("")
    
    return "\n".join(report_lines)

def extract_node_metrics_from_query_profile(query_profile: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    ã‚¯ã‚¨ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
    
    Args:
        query_profile: Databricks SQLã‚¯ã‚¨ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«JSON
        
    Returns:
        List[Dict]: ãƒãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆ
    """
    node_metrics = []
    
    graphs = query_profile.get('graphs', [])
    if not graphs:
        return node_metrics
    
    nodes = graphs[0].get('nodes', [])
    
    for node in nodes:
        # åŸºæœ¬æƒ…å ±
        node_info = {
            'node_id': node.get('id', ''),
            'name': node.get('name', ''),
            'tag': node.get('tag', ''),
            'metrics': node.get('metrics', []),
            'metadata': node.get('metadata', []),
            'key_metrics': {},
            'detailed_metrics': {}
        }
        
        # keyMetricsã®å‡¦ç†
        key_metrics = node.get('keyMetrics', {})
        if key_metrics:
            node_info['key_metrics'] = {
                'durationMs': key_metrics.get('durationMs', 0),
                'rowsNum': key_metrics.get('rowsNum', 0),
                'peakMemoryBytes': key_metrics.get('peakMemoryBytes', 0)
            }
        
        # metricsã‹ã‚‰detailed_metricsã‚’æ§‹ç¯‰
        detailed_metrics = {}
        for metric in node.get('metrics', []):
            label = metric.get('label', '')
            value = metric.get('value', 0)
            if label:
                detailed_metrics[label] = {'value': value, 'label': label}
        
        node_info['detailed_metrics'] = detailed_metrics
        node_metrics.append(node_info)
    
    return node_metrics

def analyze_query_profile_with_shuffle_optimization(json_file_path: str, output_language: str = 'ja') -> str:
    """
    ã‚¯ã‚¨ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§Shuffleæœ€é©åŒ–åˆ†æã‚’å®Ÿè¡Œ
    
    Args:
        json_file_path: JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        output_language: å‡ºåŠ›è¨€èª ('ja' or 'en')
        
    Returns:
        str: åˆ†æãƒ¬ãƒãƒ¼ãƒˆ
    """
    try:
        # JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        with open(json_file_path, 'r', encoding='utf-8') as f:
            query_profile = json.load(f)
        
        # åŸºæœ¬ã‚¯ã‚¨ãƒªæƒ…å ±
        query_info = query_profile.get('query', {})
        query_id = query_info.get('id', 'Unknown')
        status = query_info.get('status', 'Unknown')
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æƒ…å ±
        metrics = query_info.get('metrics', {})
        total_time_ms = metrics.get('totalTimeMs', 0)
        total_time_sec = total_time_ms / 1000.0
        
        # ãƒãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º
        node_metrics = extract_node_metrics_from_query_profile(query_profile)
        
        # Shuffleæ“ä½œåˆ†æ
        shuffle_analysis = analyze_shuffle_operations(node_metrics)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report_lines = []
        
        if output_language == 'ja':
            report_lines.extend([
                "ğŸ” ã‚¯ã‚¨ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æçµæœ",
                "=" * 50,
                f"ğŸ“‹ ã‚¯ã‚¨ãƒªID: {query_id}",
                f"ğŸ“Š ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {status}",
                f"â±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {total_time_sec:.2f} ç§’",
                f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«: {os.path.basename(json_file_path)}",
                ""
            ])
        else:
            report_lines.extend([
                "ğŸ” Query Profile Analysis Results",
                "=" * 50,
                f"ğŸ“‹ Query ID: {query_id}",
                f"ğŸ“Š Status: {status}",
                f"â±ï¸ Total Execution Time: {total_time_sec:.2f} seconds",
                f"ğŸ“ File: {os.path.basename(json_file_path)}",
                ""
            ])
        
        # Shuffleæœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’è¿½åŠ 
        shuffle_report = generate_shuffle_optimization_report(shuffle_analysis, output_language)
        report_lines.append(shuffle_report)
        
        return "\n".join(report_lines)
        
    except Exception as e:
        error_msg = f"Error analyzing query profile: {str(e)}\n{traceback.format_exc()}"
        return error_msg

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Enhanced Databricks SQL Profiler Analysis with Shuffle Optimization')
    parser.add_argument('json_file', help='Path to the query profile JSON file')
    parser.add_argument('--language', choices=['ja', 'en'], default='ja', help='Output language (default: ja)')
    parser.add_argument('--output', help='Output file path (default: print to stdout)')
    
    args = parser.parse_args()
    
    # åˆ†æå®Ÿè¡Œ
    result = analyze_query_profile_with_shuffle_optimization(args.json_file, args.language)
    
    # çµæœå‡ºåŠ›
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(result)
        print(f"Analysis report saved to: {args.output}")
    else:
        print(result)

print("ğŸš€ Enhanced Query Profiler Analysis Tool loaded successfully")
print("ğŸ”§ Features:")
print("  â€¢ Shuffle operation memory efficiency validation (â‰¤512MB per partition)")
print("  â€¢ Optimization priority assessment")
print("  â€¢ Actionable recommendations for performance improvement")
print("  â€¢ Cluster sizing and partition tuning advice")
print("ğŸ“Š Ready to analyze query profiles with enhanced Shuffle optimization!")