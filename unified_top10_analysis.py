# çµ±ä¸€ã•ã‚ŒãŸTop 10ãƒ—ãƒ­ã‚»ã‚¹åˆ†ææ©Ÿèƒ½
# æ—¥æœ¬èªç‰ˆã¨è‹±èªç‰ˆã®å®Œå…¨çµ±ä¸€ã‚’å®Ÿç¾

from typing import Dict, List, Any, Optional

def generate_unified_recommendations(
    memory_per_partition_mb: float,
    threshold_mb: float,
    optimal_partition_count: int,
    peak_memory_gb: float,
    duration_sec: float,
    rows_processed: int,
    output_language: str = 'ja'
) -> List[str]:
    """
    çµ±ä¸€ã•ã‚ŒãŸæ¨å¥¨äº‹é …ç”Ÿæˆé–¢æ•°
    è¨€èªã«ä¾å­˜ã—ãªã„å…±é€šãƒ­ã‚¸ãƒƒã‚¯ã§æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ
    
    Args:
        memory_per_partition_mb: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚ãŸã‚Šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡(MB)
        threshold_mb: é–¾å€¤(MB) 
        optimal_partition_count: æœ€é©ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°
        peak_memory_gb: ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡(GB)
        duration_sec: å®Ÿè¡Œæ™‚é–“(ç§’)
        rows_processed: å‡¦ç†è¡Œæ•°
        output_language: å‡ºåŠ›è¨€èª ('ja' or 'en')
    
    Returns:
        List[str]: æ¨å¥¨äº‹é …ãƒªã‚¹ãƒˆ
    """
    recommendations = []
    
    # å…±é€šã®æ¨å¥¨äº‹é …ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    templates = {
        'ja': {
            'very_high_memory': "ğŸš¨ éå¸¸ã«é«˜ã„ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ ({:.0f}MB/ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³): ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã‚’{}ä»¥ä¸Šã«å¢—åŠ ã™ã‚‹ã‹ã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚µã‚¤ã‚ºã‚’æ‹¡å¼µã—ã¦ãã ã•ã„",
            'cluster_expansion': "ğŸ–¥ï¸ ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ‹¡å¼µ: ã‚ˆã‚Šå¤šãã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ã¾ãŸã¯é«˜ãƒ¡ãƒ¢ãƒªã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ä½¿ç”¨ã‚’æ¤œè¨",
            'high_memory': "âš ï¸ é«˜ã„ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ ({:.0f}MB/ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³): ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã‚’{}ä»¥ä¸Šã«å¢—åŠ ã™ã‚‹ã“ã¨ã‚’å¼·ãæ¨å¥¨",
            'aqe_settings': "âš™ï¸ AQEè¨­å®š: spark.sql.adaptive.advisoryPartitionSizeInBytes ã®èª¿æ•´ã‚’æ¤œè¨",
            'memory_efficiency': "ğŸ’¡ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ”¹å–„ ({:.0f}MB/ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³): ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã‚’{}ã«èª¿æ•´ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨",
            'liquid_clustering': "ğŸ”§ Liquid Clusteringã®å®Ÿè£…ã«ã‚ˆã‚Šã€Shuffleæ“ä½œã®å‰Šæ¸›ã‚’æ¤œè¨ (ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {:.1f}GB)",
            'long_execution': "â±ï¸ å®Ÿè¡Œæ™‚é–“ãŒé•·ã„ ({:.1f}ç§’): ãƒ‡ãƒ¼ã‚¿åˆ†æ•£æˆ¦ç•¥ã®è¦‹ç›´ã—ã‚’æ¨å¥¨",
            'large_data': "ğŸ“Š å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç† ({:,}è¡Œ): ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆJOINã‚„äº‹å‰é›†ç´„ã®æ´»ç”¨ã‚’æ¤œè¨",
            'repartition_hint': "ğŸ”§ SQLã‚¯ã‚¨ãƒªã§ç™ºç”Ÿã—ã¦ã„ã‚‹å ´åˆã¯REPARTITONãƒ’ãƒ³ãƒˆã‚‚ã—ãã¯REPARTITON_BY_RANGEãƒ’ãƒ³ãƒˆ(Windowé–¢æ•°ä½¿ç”¨æ™‚)ã‚’é©åˆ‡ã«è¨­å®šã—ã¦ãã ã•ã„"
        },
        'en': {
            'very_high_memory': "ğŸš¨ Very high memory usage ({:.0f}MB/partition): Increase partition count to {} or more, or expand cluster size",
            'cluster_expansion': "ğŸ–¥ï¸ Cluster expansion: Consider using more worker nodes or high-memory instances",
            'high_memory': "âš ï¸ High memory usage ({:.0f}MB/partition): Strongly recommend increasing partition count to {} or more",
            'aqe_settings': "âš™ï¸ AQE settings: Consider adjusting spark.sql.adaptive.advisoryPartitionSizeInBytes",
            'memory_efficiency': "ğŸ’¡ Memory efficiency improvement ({:.0f}MB/partition): Recommend adjusting partition count to {}",
            'liquid_clustering': "ğŸ”§ Consider implementing Liquid Clustering to reduce Shuffle operations (current memory usage: {:.1f}GB)",
            'long_execution': "â±ï¸ Long execution time ({:.1f} seconds): Recommend reviewing data distribution strategy",
            'large_data': "ğŸ“Š Large data processing ({:,} rows): Consider using broadcast JOIN or pre-aggregation",
            'repartition_hint': "ğŸ”§ If occurring in SQL queries, please appropriately configure REPARTITION hints or REPARTITION_BY_RANGE hints (when using Window functions)"
        }
    }
    
    # è¨€èªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é¸æŠ
    lang_templates = templates.get(output_language, templates['ja'])
    
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã«åŸºã¥ãæ¨å¥¨äº‹é …
    if memory_per_partition_mb > threshold_mb * 4:  # 2GBä»¥ä¸Š
        recommendations.append(lang_templates['very_high_memory'].format(memory_per_partition_mb, optimal_partition_count))
        recommendations.append(lang_templates['cluster_expansion'])
    elif memory_per_partition_mb > threshold_mb * 2:  # 1GBä»¥ä¸Š
        recommendations.append(lang_templates['high_memory'].format(memory_per_partition_mb, optimal_partition_count))
        recommendations.append(lang_templates['aqe_settings'])
    elif memory_per_partition_mb > threshold_mb:  # 512MBä»¥ä¸Š
        recommendations.append(lang_templates['memory_efficiency'].format(memory_per_partition_mb, optimal_partition_count))
    
    # é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã«å¯¾ã™ã‚‹æ¨å¥¨äº‹é …
    if peak_memory_gb > 100:  # 100GBä»¥ä¸Š
        recommendations.append(lang_templates['liquid_clustering'].format(peak_memory_gb))
    
    # é•·æ™‚é–“å®Ÿè¡Œã«å¯¾ã™ã‚‹æ¨å¥¨äº‹é …
    if duration_sec > 300:  # 5åˆ†ä»¥ä¸Š
        recommendations.append(lang_templates['long_execution'].format(duration_sec))
    
    # å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã«å¯¾ã™ã‚‹æ¨å¥¨äº‹é …
    if rows_processed > 1000000000:  # 10å„„è¡Œä»¥ä¸Š
        recommendations.append(lang_templates['large_data'].format(rows_processed))
    
    # SQLã‚¯ã‚¨ãƒªæœ€é©åŒ–ãƒ’ãƒ³ãƒˆ
    if memory_per_partition_mb > threshold_mb:
        recommendations.append(lang_templates['repartition_hint'])
    
    return recommendations


def generate_unified_top10_analysis_data(
    extracted_metrics: Dict[str, Any], 
    limit_nodes: int = 10,
    output_language: str = 'ja'
) -> Dict[str, Any]:
    """
    çµ±ä¸€ã•ã‚ŒãŸTop 10ãƒ—ãƒ­ã‚»ã‚¹åˆ†æãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    å®Œå…¨ã«è¨€èªéä¾å­˜ã®å…±é€šãƒ­ã‚¸ãƒƒã‚¯
    
    Args:
        extracted_metrics: æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹
        limit_nodes: åˆ†æã™ã‚‹ãƒãƒ¼ãƒ‰æ•°
        output_language: å‡ºåŠ›è¨€èª
    
    Returns:
        Dict[str, Any]: çµ±ä¸€ã•ã‚ŒãŸåˆ†æãƒ‡ãƒ¼ã‚¿
    """
    # æ—¢å­˜ã®generate_top10_time_consuming_processes_dataé–¢æ•°ã‚’å‘¼ã³å‡ºã—
    # è¨€èªä¾å­˜éƒ¨åˆ†ã‚’çµ±ä¸€ã•ã‚ŒãŸé–¢æ•°ã§å‡¦ç†
    
    # ãƒãƒ¼ãƒ‰ã‚’å®Ÿè¡Œæ™‚é–“ã§ã‚½ãƒ¼ãƒˆ
    sorted_nodes = sorted(extracted_metrics['node_metrics'], 
                         key=lambda x: x['key_metrics'].get('durationMs', 0), 
                         reverse=True)
    
    # æŒ‡å®šã•ã‚ŒãŸãƒãƒ¼ãƒ‰æ•°ã¾ã§å‡¦ç†
    final_sorted_nodes = sorted_nodes[:limit_nodes]
    
    # çµ±ä¸€ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’åˆæœŸåŒ–
    analysis_data = {
        'summary': {
            'total_nodes_analyzed': len(final_sorted_nodes),
            'total_duration': 0,
            'total_top_nodes_duration': 0,
            'calculation_method': 'unknown',
            'output_language': output_language
        },
        'nodes': []
    }
    
    if not final_sorted_nodes:
        return analysis_data
    
    # å…¨ä½“æ™‚é–“ã®è¨ˆç®—ï¼ˆãƒ‡ã‚°ãƒ¬é˜²æ­¢ï¼‰
    overall_metrics = extracted_metrics.get('overall_metrics', {})
    total_duration = overall_metrics.get('total_time_ms', 0)
    task_total_time_ms = overall_metrics.get('task_total_time_ms', 0)
    
    if task_total_time_ms > 0:
        total_duration = task_total_time_ms
        calculation_method = 'task_total_time_ms'
    elif total_duration <= 0:
        execution_time_ms = overall_metrics.get('execution_time_ms', 0)
        if execution_time_ms > 0:
            total_duration = execution_time_ms
            calculation_method = 'execution_time_ms'
        else:
            max_node_time = max([node['key_metrics'].get('durationMs', 0) for node in sorted_nodes], default=1)
            total_duration = int(max_node_time * 1.2)
            calculation_method = 'estimated'
    else:
        calculation_method = 'total_time_ms'
    
    # å„ãƒãƒ¼ãƒ‰ã®åˆ†æ
    for i, node in enumerate(final_sorted_nodes):
        duration_ms = node['key_metrics'].get('durationMs', 0)
        rows_num = node['key_metrics'].get('rowsNum', 0)
        memory_mb = node['key_metrics'].get('peakMemoryBytes', 0) / 1024 / 1024
        
        # æ™‚é–“ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ï¼ˆãƒ‡ã‚°ãƒ¬é˜²æ­¢ï¼‰
        time_percentage = min((duration_ms / max(total_duration, 1)) * 100, 100.0)
        
        # é‡è¦åº¦ã¨ã‚¢ã‚¤ã‚³ãƒ³
        if duration_ms >= 10000:
            severity = "CRITICAL"
            time_icon = "ğŸ”´"
        elif duration_ms >= 5000:
            severity = "HIGH"
            time_icon = "ğŸŸ "
        elif duration_ms >= 1000:
            severity = "MEDIUM"
            time_icon = "ğŸŸ¡"
        else:
            severity = "LOW"
            time_icon = "ğŸŸ¢"
        
        # ãƒ¡ãƒ¢ãƒªã‚¢ã‚¤ã‚³ãƒ³
        memory_icon = "ğŸ’š" if memory_mb < 100 else "âš ï¸" if memory_mb < 1000 else "ğŸš¨"
        
        # ãƒãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹é€ åŒ–ï¼ˆå®Œå…¨ã«è¨€èªéä¾å­˜ï¼‰
        node_data = {
            'rank': i + 1,
            'node_id': node.get('node_id', node.get('id', 'N/A')),
            'node_name': node.get('name', ''),
            'duration_ms': duration_ms,
            'time_percentage': time_percentage,
            'rows_processed': rows_num,
            'memory_mb': memory_mb,
            'severity': severity,
            'icons': {
                'time': time_icon,
                'memory': memory_icon
            },
            'processing_efficiency': {
                'rows_per_sec': (rows_num * 1000) / duration_ms if duration_ms > 0 else 0
            }
        }
        
        analysis_data['nodes'].append(node_data)
    
    # ã‚µãƒãƒªãƒ¼æƒ…å ±ã‚’æ›´æ–°
    analysis_data['summary'].update({
        'total_duration': total_duration,
        'total_top_nodes_duration': sum(node['key_metrics'].get('durationMs', 0) for node in final_sorted_nodes),
        'calculation_method': calculation_method
    })
    
    return analysis_data


def format_unified_top10_report(analysis_data: Dict[str, Any], output_language: str = 'ja') -> str:
    """
    çµ±ä¸€ã•ã‚ŒãŸTop 10ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼
    å®Œå…¨ã«åŒã˜è©³ç´°åº¦ã€åˆ†æç²¾åº¦ã€æœ€é©åŒ–ææ¡ˆã‚’æä¾›
    
    Args:
        analysis_data: çµ±ä¸€ã•ã‚ŒãŸåˆ†æãƒ‡ãƒ¼ã‚¿
        output_language: å‡ºåŠ›è¨€èª
    
    Returns:
        str: ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆ
    """
    report_lines = []
    report_lines.append("")
    
    summary = analysis_data['summary']
    nodes = analysis_data['nodes']
    limit_nodes = len(nodes)
    
    # è¨€èªåˆ¥ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    templates = {
        'ja': {
            'cumulative_time': "ğŸ“Š ç´¯ç©ã‚¿ã‚¹ã‚¯å®Ÿè¡Œæ™‚é–“ï¼ˆä¸¦åˆ—ï¼‰: {:,} ms ({:.1f} æ™‚é–“)",
            'top_total_time': "ğŸ“ˆ TOP{}åˆè¨ˆæ™‚é–“ï¼ˆä¸¦åˆ—å®Ÿè¡Œï¼‰: {:,} ms",
            'execution_time': "**å®Ÿè¡Œæ™‚é–“**: {:,}ms ({:.1f}% of total)",
            'severity': "**é‡è¦åº¦**: {}",
            'detailed_metrics': "**ğŸ“Š è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹:**",
            'exec_time': "- â±ï¸  å®Ÿè¡Œæ™‚é–“: {:>8,} ms ({:>6.1f} sec)",
            'rows_processed': "- ğŸ“Š å‡¦ç†è¡Œæ•°: {:>8,} è¡Œ",
            'peak_memory': "- ğŸ’¾ ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª: {:>6.1f} MB",
            'processing_efficiency': "- ğŸš€ å‡¦ç†åŠ¹ç‡: {:>8,.0f} è¡Œ/ç§’",
            'node_id': "- ğŸ†” ãƒãƒ¼ãƒ‰ID: {}",
            'no_metrics': "âš ï¸ ãƒãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
        },
        'en': {
            'cumulative_time': "ğŸ“Š Cumulative task execution time (parallel): {:,} ms ({:.1f} hours)",
            'top_total_time': "ğŸ“ˆ TOP{} total time (parallel execution): {:,} ms",
            'execution_time': "**Execution Time**: {:,}ms ({:.1f}% of total)",
            'severity': "**Severity**: {}",
            'detailed_metrics': "**ğŸ“Š Detailed Metrics:**",
            'exec_time': "- â±ï¸  Execution time: {:>8,} ms ({:>6.1f} sec)",
            'rows_processed': "- ğŸ“Š Rows processed: {:>8,} rows",
            'peak_memory': "- ğŸ’¾ Peak memory: {:>6.1f} MB",
            'processing_efficiency': "- ğŸš€ Processing efficiency: {:>8,.0f} rows/sec",
            'node_id': "- ğŸ†” Node ID: {}",
            'no_metrics': "âš ï¸ No node metrics found"
        }
    }
    
    # è¨€èªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é¸æŠ
    lang_templates = templates.get(output_language, templates['ja'])
    
    if nodes:
        report_lines.append(lang_templates['cumulative_time'].format(
            summary['total_duration'], summary['total_duration']/3600000
        ))
        report_lines.append(lang_templates['top_total_time'].format(
            limit_nodes, summary['total_top_nodes_duration']
        ))
        report_lines.append("")
        
        for node in nodes:
            # åŸºæœ¬æƒ…å ±è¡¨ç¤º
            icons = node['icons']
            severity = node['severity']
            node_name = node['node_name'][:100] + "..." if len(node['node_name']) > 100 else node['node_name']
            
            report_lines.append(f"### {node['rank']}. {icons['time']}{icons['memory']} [{severity:8}] {node_name}")
            report_lines.append("")
            report_lines.append(lang_templates['execution_time'].format(
                node['duration_ms'], node['time_percentage']
            ))
            report_lines.append(lang_templates['severity'].format(severity))
            report_lines.append("")
            report_lines.append(lang_templates['detailed_metrics'])
            report_lines.append(lang_templates['exec_time'].format(
                node['duration_ms'], node['duration_ms']/1000
            ))
            report_lines.append(lang_templates['rows_processed'].format(node['rows_processed']))
            report_lines.append(lang_templates['peak_memory'].format(node['memory_mb']))
            
            # å‡¦ç†åŠ¹ç‡
            efficiency = node['processing_efficiency']['rows_per_sec']
            if efficiency > 0:
                report_lines.append(lang_templates['processing_efficiency'].format(efficiency))
            
            # ãƒãƒ¼ãƒ‰ID
            report_lines.append(lang_templates['node_id'].format(node['node_id']))
            report_lines.append("")
    else:
        report_lines.append(lang_templates['no_metrics'])
    
    return "\n".join(report_lines)


# å®Œå…¨çµ±ä¸€ã•ã‚ŒãŸãƒ¡ã‚¤ãƒ³é–¢æ•°
def generate_completely_unified_top10_report(
    extracted_metrics: Dict[str, Any], 
    limit_nodes: int = 10, 
    output_language: str = None
) -> str:
    """
    å®Œå…¨ã«çµ±ä¸€ã•ã‚ŒãŸTop 10ãƒ—ãƒ­ã‚»ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    
    OUTPUT_LANGUAGEã®è¨­å®šã«é–¢ä¿‚ãªãï¼š
    - å®Œå…¨ã«åŒã˜è©³ç´°åº¦
    - å®Œå…¨ã«åŒã˜åˆ†æç²¾åº¦  
    - å®Œå…¨ã«åŒã˜æœ€é©åŒ–ææ¡ˆ
    ã‚’æä¾›
    
    Args:
        extracted_metrics: æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹
        limit_nodes: è¡¨ç¤ºã™ã‚‹ãƒãƒ¼ãƒ‰æ•°
        output_language: å‡ºåŠ›è¨€èª ('ja' or 'en')
    
    Returns:
        str: çµ±ä¸€ã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆ
    """
    if output_language is None:
        import sys
        if 'globals' in dir(sys.modules.get('__main__', sys.modules[__name__])):
            output_language = globals().get('OUTPUT_LANGUAGE', 'ja')
        else:
            output_language = 'ja'
    
    # çµ±ä¸€ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ç”Ÿæˆ
    analysis_data = generate_unified_top10_analysis_data(extracted_metrics, limit_nodes, output_language)
    
    # çµ±ä¸€ã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    return format_unified_top10_report(analysis_data, output_language)